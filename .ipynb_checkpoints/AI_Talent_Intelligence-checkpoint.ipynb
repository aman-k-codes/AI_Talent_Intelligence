{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a72388c-3ca2-479b-a448-a9e51ee3cdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"resume/Aman_Kumar_Resume_entry_lvl_mld.pdf\",\n",
    "    \"resume/resume (1).pdf\",\n",
    "    \"resume/resume (2).pdf\",\n",
    "    \"resume/resume (3).pdf\",\n",
    "    \"resume/resume (4).pdf\",\n",
    "    \"resume/resume (5).pdf\",\n",
    "    \"resume/resume (6).pdf\",\n",
    "    \"resume/resume (7).pdf\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba5910be-1dbd-43ac-aaa4-7a82fc42c294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import re\n",
    "import fitz\n",
    "import spacy\n",
    "import unicodedata\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "\n",
    "\n",
    "def get_refined_skills(text):\n",
    "    doc = nlp(text)\n",
    "    matcher = PhraseMatcher(nlp.vocab)\n",
    "    \n",
    "    # List of skills you want to ensure are caught\n",
    "    skill_list = [\"Python\", \"Machine Learning\", \"REST API\", \"Docker\", \"Flask\", \"Laravel\", \"MySQL\"]\n",
    "    patterns = [nlp.make_doc(text) for text in skill_list]\n",
    "    matcher.add(\"SKILLS\", patterns)\n",
    "    \n",
    "    matches = matcher(doc)\n",
    "    found_skills = set([doc[start:end].text for match_id, start, end in matches])\n",
    "    \n",
    "    return found_skills\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "candidateRawData = {\n",
    "    \"name\": None,\n",
    "    \"skills\": [],\n",
    "    \"resume_text\": \"\",\n",
    "    \"links\": [],\n",
    "    \"github\": None,\n",
    "}\n",
    "github_details = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db5ba725-8045-4b50-994d-919c8b199cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[–—−]\", \"-\", text)\n",
    "    text = re.sub(r\"[•●▪►■·]\", \" \", text)\n",
    "    text = re.sub(r\"[\\x00-\\x1f\\x7f-\\x9f]\", \" \", text)\n",
    "    text = re.sub(r\"[^a-z0-9\\.\\-\\+\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b1111f4-9828-472c-8214-6e1be484c67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(path):\n",
    "    pages = fitz.open(path)\n",
    "    text = ''\n",
    "    links = []\n",
    "\n",
    "    for page in pages:\n",
    "        text += page.get_text()\n",
    "        for url in page.get_links():\n",
    "            if 'uri' in url:\n",
    "                links.append(url['uri'])\n",
    "\n",
    "    return text, links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72e8756e-793a-476a-b60a-22c2468459a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_github_links(links):\n",
    "    github_links = set()\n",
    "\n",
    "    for link in links:\n",
    "        if link and \"github.com\" in link:\n",
    "            github_links.add(link.split(\"?\")[0])\n",
    "\n",
    "    return list(github_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10407814-2968-49c0-a354-b37805e8920f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_github_username(url):\n",
    "    return url.rstrip(\"/\").split(\"github.com/\")[-1].split(\"/\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3358652d-69d0-4f61-ad4b-283889f305cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_github_profile(username, timeout=10):\n",
    "    \"\"\"\n",
    "    Fetch GitHub user profile. Token can be passed or provided via\n",
    "    GITHUB_TOKEN or GH_TOKEN env var. Returns JSON on success or raises an HTTPError.\n",
    "    \"\"\"\n",
    "    gittoken = os.environ.get(\"GITHUB_TOKEN\")\n",
    "    print(gittoken)\n",
    "    headers = {\n",
    "        \"Accept\": \"application/vnd.github+json\",\n",
    "        \"User-Agent\": \"my-script-or-app\",  # set a descriptive user agent\n",
    "    }\n",
    "    if gittoken:\n",
    "        headers[\"Authorization\"] = f\"Bearer {gittoken}\"  # or \"token {token}\"\n",
    "\n",
    "    url = f\"https://api.github.com/users/{username}\"\n",
    "    resp = requests.get(url, headers=headers, timeout=timeout)\n",
    "\n",
    "    # Raise if HTTP status indicates an error (e.g., 404, 401, 403)\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17416cbd-827b-4732-83e0-800221cd5867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resume_name(text):\n",
    "    tokens = nlp(text)\n",
    "    for token in tokens.ents:\n",
    "        if token.label_ == 'PERSON':\n",
    "            return token\n",
    "        else:\n",
    "            return null\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79909b1c-98b6-42aa-8e27-0a6b6196855f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SKILLS = {\n",
    "    # Languages & Frameworks\n",
    "    \"python\", \"java\", \"php\", \"laravel\", \"sql\", \"mysql\", \"javascript\", \"typescript\",\n",
    "    \"flask\", \"django\", \"fastapi\", \"node.js\", \"react\", \"vue\",\n",
    "    \n",
    "    # Machine Learning & AI\n",
    "    \"ml\", \"machine learning\", \"nlp\", \"natural language processing\", \"deep learning\", \n",
    "    \"computer vision\", \"pytorch\", \"tensorflow\", \"keras\", \"scikit-learn\", \"pandas\", \n",
    "    \"numpy\", \"opencv\", \"huggingface\", \"llm\", \"bert\", \"transformers\",\n",
    "    \n",
    "    # Backend & Infrastructure\n",
    "    \"backend\", \"rest\", \"api\", \"rest api\", \"graphql\", \"microservices\", \"docker\", \n",
    "    \"kubernetes\", \"aws\", \"gcp\", \"azure\", \"linux\", \"git\", \"postman\", \"redis\", \n",
    "    \"rabbitmq\", \"celery\", \"nginx\", \"ci/cd\",\n",
    "    \n",
    "    # Data & Database\n",
    "    \"postgresql\", \"mongodb\", \"sqlite\", \"elasticsearch\", \"data engineering\", \n",
    "    \"data science\", \"web scraping\", \"beautifulsoup\", \"selenium\"\n",
    "}\n",
    "\n",
    "def get_resume_skills(text):\n",
    "    doc = nlp(text.lower())\n",
    "    found_skills = set()\n",
    "\n",
    "    for token in doc:\n",
    "        if token.text in SKILLS:\n",
    "            found_skills.add(token.text)\n",
    "\n",
    "    return list(found_skills)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70f1eae3-b98e-4e41-a4fa-86d1a3811b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawText, rawLinks = extract_text_from_pdf(\"resume/Aman_Kumar_Resume_entry_lvl_mld.pdf\")\n",
    "candidateRawData[\"resume_text\"] = rawText\n",
    "candidateRawData[\"links\"] = rawLinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d114df58-dcd2-4dfe-8624-90cc1b1096ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "github_url = extract_github_links(rawLinks)\n",
    "github_username = get_github_username(github_url[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea1e17b8-fab4-4c2a-84bf-8c2289c20158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'login': 'aman-k-codes', 'id': 76843794, 'node_id': 'MDQ6VXNlcjc2ODQzNzk0', 'avatar_url': 'https://avatars.githubusercontent.com/u/76843794?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/aman-k-codes', 'html_url': 'https://github.com/aman-k-codes', 'followers_url': 'https://api.github.com/users/aman-k-codes/followers', 'following_url': 'https://api.github.com/users/aman-k-codes/following{/other_user}', 'gists_url': 'https://api.github.com/users/aman-k-codes/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/aman-k-codes/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/aman-k-codes/subscriptions', 'organizations_url': 'https://api.github.com/users/aman-k-codes/orgs', 'repos_url': 'https://api.github.com/users/aman-k-codes/repos', 'events_url': 'https://api.github.com/users/aman-k-codes/events{/privacy}', 'received_events_url': 'https://api.github.com/users/aman-k-codes/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False, 'name': 'Aman Kumar Sahu', 'company': None, 'blog': 'https://codeforcrack.com', 'location': 'Bangaluru, Karnataka', 'email': None, 'hireable': True, 'bio': 'Backend Engineer | Laravel & Python | Building REST APIs and ML-powered tools', 'twitter_username': None, 'public_repos': 4, 'public_gists': 0, 'followers': 3, 'following': 1, 'created_at': '2021-01-01T15:10:20Z', 'updated_at': '2026-01-18T07:38:21Z'}\n"
     ]
    }
   ],
   "source": [
    "github_profile = get_github_profile(github_username)\n",
    "candidateRawData[\"github\"] = github_profile\n",
    "print(candidateRawData['github'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0636e4aa-cd49-49e8-bc61-40c3580be761",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidateRawData['github']['followers_url'] = requests.get(candidateRawData['github']['followers_url']).json()\n",
    "candidateRawData['github']['following_url'] = requests.get(candidateRawData['github']['following_url']).json()\n",
    "candidateRawData['github']['gists_url'] = requests.get(candidateRawData['github']['gists_url']).json()\n",
    "candidateRawData['github']['starred_url'] = requests.get(candidateRawData['github']['starred_url']).json()\n",
    "candidateRawData['github']['subscriptions_url'] = requests.get(candidateRawData['github']['subscriptions_url']).json()\n",
    "candidateRawData['github']['organizations_url'] = requests.get(candidateRawData['github']['organizations_url']).json()\n",
    "candidateRawData['github']['repos_url'] = requests.get(candidateRawData['github']['repos_url']).json()\n",
    "candidateRawData['github']['events_url'] = requests.get(candidateRawData['github']['events_url']).json()\n",
    "candidateRawData['github']['received_events_url'] = requests.get(candidateRawData['github']['received_events_url']).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51c69958-75a7-4be4-9e31-ea187bfdf21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidateRawData['name'] = get_resume_name(candidateRawData['resume_text'])\n",
    "# candidateRawData['resume_text'] = normalize_text(candidateRawData['resume_text'])\n",
    "candidateRawData['skills'] = get_resume_skills(candidateRawData['resume_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a44e042-6dcb-4c6a-9db1-690b1e776eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flask', 'sql', 'javascript', 'rest', 'pytorch', 'git', 'ml', 'nlp', 'docker', 'postman', 'mysql', 'laravel', 'python', 'php', 'backend', 'fastapi', 'api', 'linux']\n"
     ]
    }
   ],
   "source": [
    "print(candidateRawData['skills'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff8a549c-ad3f-49ca-bee1-080b8f5a9093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMAN KUMAR\n",
      "Machine Learning Engineer — Backend Systems\n",
      "amansahu.er@gmail.com · +91-8319151766 · LinkedIn · GitHub\n",
      "PROFESSIONAL SUMMARY\n",
      "Software Engineer with 2+ years of experience building scalable backend systems and integrating machine learning\n",
      "features into production applications. Hands-on experience in deploying Python-based ML services, designing REST APIs,\n",
      "and managing end-to-end document and image processing workflows. Strong backend foundation with growing expertise\n",
      "in applied machine learning.\n",
      "TECHNICAL SKILLS\n",
      "• Machine Learning: Model inference, basic NLP workflows, image processing, Scikit-learn, PyTorch (fundamentals)\n",
      "• Backend & Systems: REST API design, Flask, FastAPI, Laravel, system integration\n",
      "• Programming: Python, PHP, SQL, JavaScript\n",
      "• Tools: Docker, Git, Linux, Postman\n",
      "PROFESSIONAL EXPERIENCE\n",
      "Software Engineer\n",
      "Jul 2024 – Present\n",
      "ISKCON Bangalore\n",
      "Bangalore, India\n",
      "• Developed and maintained backend services supporting 50k+ users, focusing on reliability and data consistency.\n",
      "• Implemented strong request validation and structured data handling, supporting downstream analytics and ML usage.\n",
      "• Optimized MySQL queries and indexing, reducing API response times by 30%.\n",
      "• Designed modular services enabling independent scaling of compute-heavy tasks.\n",
      "Software Engineer\n",
      "Jun 2023 – Jun 2024\n",
      "FixingDots\n",
      "Raipur, India\n",
      "• Built FixHR, an internal HR management system, automating attendance, reporting, and employee records.\n",
      "• Reduced manual administrative work by improving data workflows and report generation.\n",
      "• Participated in full product lifecycle including development, deployment, and post-release support.\n",
      "PROJECTS\n",
      "CraftMyDoc — Document Automation Platform\n",
      "Python, Flask, Laravel\n",
      "• Built a document automation platform providing resume creation, file conversion, and image processing features.\n",
      "• Developed Python-based ML services using Flask for image background removal and enhancement, exposed through\n",
      "REST APIs.\n",
      "• Integrated ML services with a Laravel backend for request orchestration, authentication, and secure file handling.\n",
      "• Designed end-to-end workflows for file upload, processing, and download with robust error handling.\n",
      "• Focused on production readiness, including input validation and API performance optimization.\n",
      "Basil Woods School ERP\n",
      "Laravel, MySQL\n",
      "• Developed a school management system with a focus on data integrity, transactional consistency, and reporting.\n",
      "EDUCATION\n",
      "Chhattisgarh Swami Vivekanand Technical University\n",
      "Bhilai, India\n",
      "Bachelor of Technology in Computer Science and Engineering\n",
      "2019 – 2023\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(candidateRawData['resume_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
