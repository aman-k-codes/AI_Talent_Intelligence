{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a72388c-3ca2-479b-a448-a9e51ee3cdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"resume/Aman_Kumar_Resume_entry_lvl_mld.pdf\",\n",
    "    \"resume/resume (1).pdf\",\n",
    "    \"resume/resume (2).pdf\",\n",
    "    \"resume/resume (3).pdf\",\n",
    "    \"resume/resume (4).pdf\",\n",
    "    \"resume/resume (5).pdf\",\n",
    "    \"resume/resume (6).pdf\",\n",
    "    \"resume/resume (7).pdf\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1de589-a246-44be-bda1-52e7ea3790bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import fitz\n",
    "import spacy\n",
    "import requests\n",
    "import unicodedata\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Load NLP model globally to avoid reloading\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "class ResumeParsingPipeline:\n",
    "    def __init__(self, github_token=None):\n",
    "        self.github_token = github_token or os.environ.get(\"GITHUB_TOKEN\")\n",
    "        self.skills_list = [\n",
    "            \"python\", \"java\", \"php\", \"laravel\", \"sql\", \"mysql\", \"javascript\", \n",
    "            \"typescript\", \"flask\", \"django\", \"fastapi\", \"node.js\", \"react\", \n",
    "            \"docker\", \"kubernetes\", \"aws\", \"ml\", \"machine learning\", \"nlp\"\n",
    "        ]\n",
    "        self.matcher = self._build_matcher()\n",
    "\n",
    "    def _build_matcher(self):\n",
    "        matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "        patterns = [nlp.make_doc(text) for text in self.skills_list]\n",
    "        matcher.add(\"SKILLS\", patterns)\n",
    "        return matcher\n",
    "\n",
    "    # --- Utility Methods ---\n",
    "    def normalize_text(self, text):\n",
    "        if not text: return \"\"\n",
    "        text = unicodedata.normalize(\"NFKD\", text).lower()\n",
    "        text = re.sub(r\"[‚Äì‚Äî‚àí]\", \"-\", text)\n",
    "        text = re.sub(r\"[‚Ä¢‚óè‚ñ™‚ñ∫‚ñ†¬∑]\", \" \", text)\n",
    "        text = re.sub(r\"[^a-z0-9\\.\\-\\+\\s]\", \" \", text)\n",
    "        return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    def call_github_api(self, url):\n",
    "        headers = {\"Accept\": \"application/vnd.github+json\"}\n",
    "        if self.github_token:\n",
    "            headers[\"Authorization\"] = f\"Bearer {self.github_token}\"\n",
    "        try:\n",
    "            resp = requests.get(url, headers=headers, timeout=10)\n",
    "            resp.raise_for_status()\n",
    "            return resp.json()\n",
    "        except Exception as e:\n",
    "            print(f\"Error calling API {url}: {e}\")\n",
    "            return None\n",
    "\n",
    "    # --- Extraction Stages ---\n",
    "    def extract_from_pdf(self, pdf_path):\n",
    "        doc = fitz.open(pdf_path)\n",
    "        text, links = \"\", []\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "            links.extend([l['uri'] for l in page.get_links() if 'uri' in l])\n",
    "        return text, links\n",
    "\n",
    "    def parse_github_data(self, links):\n",
    "        github_url = next((l for l in links if \"github.com\" in l), None)\n",
    "        if not github_url: return None\n",
    "\n",
    "        username = github_url.rstrip(\"/\").split(\"/\")[-1]\n",
    "        profile = self.call_github_api(f\"https://api.github.com/users/{username}\")\n",
    "        \n",
    "        if profile:\n",
    "            repos_data = self.call_github_api(profile['repos_url'])\n",
    "            return {\n",
    "                \"username\": username,\n",
    "                \"profile_url\": github_url,\n",
    "                \"followers\": profile.get(\"followers\"),\n",
    "                \"repos\": [{\"name\": r[\"name\"], \"lang\": r[\"language\"]} for r in repos_data[:5]] # Top 5\n",
    "            }\n",
    "        return None\n",
    "\n",
    "    def extract_nlp_entities(self, text):\n",
    "        doc = nlp(text)\n",
    "        normalized = self.normalize_text(text)\n",
    "        \n",
    "        # Extract Name\n",
    "        name = next((ent.text for ent in doc.ents if ent.label_ == \"PERSON\"), \"Unknown\")\n",
    "        \n",
    "        # Extract Skills using Matcher\n",
    "        matches = self.matcher(doc)\n",
    "        skills = list(set([doc[start:end].text.lower() for _, start, end in matches]))\n",
    "        \n",
    "        # Extract Education & Experience (Regex based)\n",
    "        edu_keywords = [\"bachelor\", \"b.tech\", \"master\", \"m.tech\", \"phd\", \"bsc\"]\n",
    "        education = [line for line in text.split('\\n') if any(x in line.lower() for x in edu_keywords)]\n",
    "        \n",
    "        return {\n",
    "            \"name\": name,\n",
    "            \"skills\": skills,\n",
    "            \"education\": education,\n",
    "            \"address\": list(set([ent.text for ent in doc.ents if ent.label_ in [\"GPE\", \"LOC\"]]))\n",
    "        }\n",
    "\n",
    "    # --- The Pipeline Runner ---\n",
    "    def run(self, pdf_path):\n",
    "        print(f\"üöÄ Processing: {pdf_path}\")\n",
    "        \n",
    "        # 1. Extraction\n",
    "        raw_text, links = self.extract_from_pdf(pdf_path)\n",
    "        \n",
    "        # 2. NLP Analysis\n",
    "        nlp_data = self.extract_nlp_entities(raw_text)\n",
    "        \n",
    "        # 3. Enrichment\n",
    "        github_info = self.parse_github_data(links)\n",
    "        \n",
    "        # 4. Final Object\n",
    "        return {\n",
    "            \"candidate_name\": nlp_data[\"name\"],\n",
    "            \"contact\": {\"links\": links, \"address\": nlp_data[\"address\"]},\n",
    "            \"qualifications\": {\n",
    "                \"skills\": nlp_data[\"skills\"],\n",
    "                \"education\": nlp_data[\"education\"]\n",
    "            },\n",
    "            \"external_profiles\": {\"github\": github_info},\n",
    "            \"metadata\": {\"raw_text_length\": len(raw_text)}\n",
    "        }\n",
    "\n",
    "# --- Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    pipeline = ResumeParsingPipeline()\n",
    "    result = pipeline.run(\"resume/Aman_Kumar_Resume_entry_lvl_mld.pdf\")\n",
    "    \n",
    "    import json\n",
    "    print(json.dumps(result, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
